version: '3.9'

services:
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.15.1
    container_name: mlflow
    command: >
      mlflow server
      --host 0.0.0.0
      --port 5000
      --backend-store-uri sqlite:///mlflow.db
      --artifacts-destination /mlruns
    volumes:
      - ./mlflow_data:/mlruns
    ports:
      - "5000:5000"
    restart: unless-stopped

  ai-learning-assistant:
    build: .
    ports:
      - "8888:8888"
    environment:
      - FLASK_ENV=production
      - PORT=8888
      - HOST=0.0.0.0
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - MLFLOW_EXPERIMENT_NAME=chat-api
    env_file:
      - .env
    restart: unless-stopped
    depends_on:
      - mlflow
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:8888/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # 可选：添加Nginx反向代理
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - ai-learning-assistant
    restart: unless-stopped